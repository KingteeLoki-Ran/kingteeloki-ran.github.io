<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Exploring for the first time the possibility of using implicit neural representations for autonomous 3D scene reconstruction.">
    <meta name="author" content="Yunlong Ran,
                                Jing zeng,
                                Shibo He,
                                Jiming Chen,
                                Lincheng Li,
                                Yingfeng Chen,
                                Gim Hee Lee,
                                Qi Ye">

    <title>NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction with Implicit Neural Representations</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/icon.svg" type="image/svg">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">NeurAR</h1>
<!--    <h2 class="nerf_title_v2">Tensorial Radiance Fields</h2>-->
    <h1 class="nerf_subheader_v2">Neural Uncertainty for Autonomous 3D Reconstruction with Implicit Neural Representations</h1>
    <hr>
    <p class="authors">
        <a href="https://kingteeloki-ran.github.io/"> Yunlong Ran</a>,
        <a href="https://small-zeng.github.io/"> Jing zeng</a>,
        <a href="https://person.zju.edu.cn/en/shibohe"> Shibo He</a>,
        <a href="https://person.zju.edu.cn/en/jmchen"> Jiming Chen</a>,
        <a href=""> Lincheng Li</a>,
        <a href=""> Yingfeng Chen</a>,
        <a href="https://www.comp.nus.edu.sg/~leegh/"> Gim Hee Lee</a>,
        <a href="https://person.zju.edu.cn/en/yeqi"> Qi Ye</a>
    </p>

    <!-- <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Equal Contribution</div> -->

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2207.10985.pdf">Paper</a>
        <a class="btn btn-primary" href="https://github.com/KingteeLoki-Ran/NeurAR">Code</a>
    </div>
</div>



<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Overview Video</h2>
<!--    <div class="section">-->
        <div class="vcontainer">
            <iframe class='video' src="img/neurar.mp4" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>

    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <div class="columns-5 w-row">
                <img src="img/arc.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
            <p class="paragraph-3 nerf_text">
                Implicit neural representations have shown compelling results in
                offline 3D reconstruction and also recently demonstrated the potential
                for online SLAM systems. However, applying them to autonomous 3D
                reconstruction, where robots are required to explore a scene and plan a
                view path for the reconstruction, has not been studied. In this paper,
                we explore for the first time the possibility of using implicit neural
                representations for autonomous 3D scene reconstruction by addressing
                two key challenges: 1) seeking a criterion to measure the quality of
                the candidate viewpoints for the view planning based on the new
                representations, and 2) learning the criterion from data that can
                generalize to different scenes instead of hand-crafting one. For the
                first challenge, a proxy of Peak Signal-to-Noise Ratio (PSNR) is
                proposed to quantify a viewpoint quality. The proxy is acquired by
                treating the color of a spatial point in a scene as a random variable
                under a Gaussian distribution rather than a deterministic one; the
                variance of the distribution quantifies the uncertainty of the
                reconstruction and composes the proxy. For the second challenge, the
                proxy is optimized jointly with the parameters of an implicit neural
                network for the scene. With the proposed view quality criterion, we can
                then apply the new representations to autonomous 3D reconstruction. Our
                method demonstrates significant improvements on various metrics for the
                rendered image quality and the geometry quality of the reconstructed 3D
                models when compared with variants using TSDF or reconstruction without
                view planning.
            </p>
            
            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                We assume the color to regress for a spatial point in a scene as a random variable modeled by a Gaussian distribution.  
                The Gaussian distribution models the uncertainty of the reconstruction and the variance quantifies the uncertainty.
                When the regression network converges, the variance of the distribution is given by the squared error of the predicted color and the ground truth color; the integral of the uncertainty of points in the frustum of a viewpoint can be taken as a proxy of PSNR to measure the quality of candidate viewpoints.
            </p>
            <div class="columns-5 w-row">
                <img src="img/relationship.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
            <p class="paragraph-3 nerf_text">
                a: loss curves of ray-set based, Ratio is uncertainty/MSE.
                b: our ray-set based uncertainty formulation.
                c: altenating single-ray based uncertainty formulation, work but not good as ray-set based.
                Please refer to our paper for more formulation derails.
                Under the guidiance of uncertainty, NeurAR can easily do active 3D reconstruction.
            </p>
        </div>
    </div>

</br></br>
    <div class="section">
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
            @article{ran2023neurar,
                title={NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction 
                    With Implicit Neural Representations},
                author={Ran, Yunlong and Zeng, Jing and He, Shibo and Chen, Jiming and Li, Lincheng and Chen, 
                    Yingfeng and Lee, Gimhee and Ye, Qi},
                journal={IEEE Robotics and Automation Letters},
                volume={8},
                number={2},
                pages={1125--1132},
                year={2023},
                publisher={IEEE}
              }
        </div>
    </div>

        

        

    <hr>

    <footer>
        <p>This website is partially borrowed from TensoRF.
            Send feedback and questions to <a href="https://kingteeloki-ran.github.io/">Yunlong Ran</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
